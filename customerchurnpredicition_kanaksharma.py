# -*- coding: utf-8 -*-
"""CustomerChurnPredicition_KanakSharma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZK6NvAlv3bIRZZhDutI-z-66FsLj2IKC
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libaries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Loading data file into data
data=pd.read_csv('data.csv')
data.head()

# counting the missing values in columns
data.isna().sum()

# identifying the rows and columns in data frame
data.shape

# column  name
data.columns

# dropping column customerID
data=data.drop(["customerID"],axis=1)
data.head()

# Adding column Total charges
data[data["TotalCharges"]==' ']

# converts the 'TotalCharges' column in the DataFrame data to numeric values
#The parameter errors='coerce' tells Pandas to convert invalid parsing to NaN (Not a Number).
data['TotalCharges']=pd.to_numeric(data.TotalCharges,errors='coerce')
data.isna().sum()

#It include only those rows where the value in the tenture column is 0
data[data["tenure"]==0]

# including those where tenture is non-zero
data=data[data["tenure"]!=0]

# filling missing values in column total charge with mean
data["TotalCharges"].fillna(data["TotalCharges"].mean(),inplace=True)

# finding unique values
data['SeniorCitizen'].nunique()

# changing categorical to numeric values
data['SeniorCitizen']=data['SeniorCitizen'].map({0:"No",1:"Yes"})
data.head()

# fetching  all numeric columns of dataset into one
numerical_cols=['tenure','MonthlyCharges','TotalCharges']
data[numerical_cols].describe()

#prepare the data
type=["No","Yes"]
churn_counts=data['Churn'].value_counts()

# plot the pie chart
plt.figure(figsize=(6,6))
plt.pie(churn_counts, labels=type, autopct='%1.1f%%',
        colors=sns.color_palette('pastel'),wedgeprops={'edgecolor':'black'})
plt.title("Chrun Distribution")
plt.show()

#create the cross table
contingency_table=pd.crosstab(data['Churn'],data['gender'])
#display the cross tabLE
print(contingency_table)

#calculate proportions
proportions = data.groupby(['Churn', 'gender']).size().groupby(level=0).apply(lambda x: x/x.sum()).reset_index(name='proportion')

#create the countplot
plt.figure(figsize=(8, 6))
sns.barplot(x='Churn',y='proportion',hue='gender',data=proportions)

#add title and labels
plt.title('Churn by Gender')
plt.xlabel('Churn')
plt.ylabel('Proportion')

#show the plot
plt.show()

# performing the chi2 test to understand the correlation between churn and gender
from scipy.stats import chi2_contingency

contingency_table = pd.crosstab(data['Churn'], data['gender'])

#perform the test
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

#print the values
print("Chi-squared statistic:",chi2_stat)
print("P-value:",p_val)
print(expected)

#Grouping them
proportions = data.groupby(['Churn','Contract']).size().groupby(level=0).apply(lambda x:x/x.sum()).reset_index(name='proportion')

plt.figure(figsize=(8,6))
sns.barplot(x='Churn',y='proportion',hue='Contract',data=proportions,
palette='pastel')

plt.title('Customer Contract Distribution')
plt.xlabel('Churn')
plt.ylabel('proportion')

#show the plot
plt.show()

# performing chi square test
contingency_table= pd.crosstab(data['Churn'],data['Contract'])

chi2_stat,p_val,dof,expected= chi2_contingency(contingency_table)

# print the results
print("Chi- squared statistics :",chi2_stat)
print("P-value:",p_val)

# getting uniques and there count in PaymentMethod column
labels=data['PaymentMethod'].unique()
values=data['PaymentMethod'].value_counts()
values

# kde plot
ax = sns.kdeplot(data['MonthlyCharges'][data["Churn"] =='No'], color="Red",
                 shade = True)
ax = sns.kdeplot(data['MonthlyCharges'][data["Churn"] == 'Yes'],ax =ax,
                 color="Blue", shade=True)
ax.legend(["Not Churn","Churn"],loc='upper right')
ax.set_ylabel('Density');
ax.set_xlabel('Monthly Charges');
ax.set_title('Distribution of monthly charges y churn')

# kde plot
ax = sns.kdeplot(data['TotalCharges'][data["Churn"] =='No'], color="Red",
                 shade = True)
ax = sns.kdeplot(data['TotalCharges'][data["Churn"] == 'Yes'],ax =ax,
                 color="Blue", shade=True)
ax.legend(["Not Churn","Churn"],loc='upper right')
ax.set_ylabel('Density');
ax.set_xlabel('Total Charges');
ax.set_title('Distribution of monthly charges by churn')

#create the box plot
plt.figure(figsize=(10, 8))
sns.boxplot(x='Churn',y='tenure',data=data)

#add title
plt.title('Tenure vs Churn')
plt.xlabel('Churn')
plt.ylabel('Tenure(Months)')

#show the plot
plt.show()

# Define numerical columns for distribution plotting
num_cols = ["tenure", "MonthlyCharges", "TotalCharges"]
# Plot distribution for each numerical feature
plt. figure (figsize=(18, 5))
for i, feat in enumerate (num_cols) :
  plt.subplot (1, 3, i + 1)
  sns.histplot(data[feat] .dropna(), kde=True, bins=30) # Drop NA values for plotting
  plt.title(f'Distribution of {feat}')
  plt.xlabel (feat)
  plt.ylabel( 'Density')
plt. show()

# Identify categorical columns (excluding 'customerID' as it's a unique identifier)
categorical_columns = data.select_dtypes(include=['object']). columns

# Create dummy variables for categorical columns
data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)

data.head()

# Define features (X) and target variable (y)
X = data.drop (['Churn_Yes'], axis=1)
y = data ['Churn_Yes']

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2,random_state=42)

# Standardize the numerical features
scaler = StandardScaler()
numerical_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']
X_train [numerical_columns] = scaler.fit_transform(X_train [numerical_columns])
X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])

# Confirm the first few rows of the scaled training data
X_train. head ()

#Initialize the logistic regression model
logreg = LogisticRegression(max_iter=1000, random_state=42)

# Fit the model to the training data
logreg.fit(X_train, Y_train)

# Predict on the testing set
y_pred = logreg.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(Y_test , y_pred)

# Generate a confusion matrix
conf_matrix = confusion_matrix(Y_test , y_pred)

accuracy, conf_matrix

# Initialize the logistic regression model
logreg = LogisticRegression(max_iter=1000, random_state=42)

# Fit the model to the training data
logreg. fit (X_train, Y_train)

# Predict on the testing set
y_pred = logreg.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(Y_test, y_pred)

# Generate a confusion matrix
conf_matrix = confusion_matrix(Y_test , y_pred)

accuracy, conf_matrix

#Initialize the decision tree and random forest models
decision_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(random_state=42, n_jobs=-1)

# Fit the models to the training data
decision_tree.fit(X_train,Y_train)
random_forest.fit(X_train, Y_train)

# Predict on the training and testing sets
y_pred_train_dt = decision_tree.predict(X_train)
y_pred_test_dt = decision_tree.predict(X_test)
y_pred_train_rf = random_forest.predict(X_train)
y_pred_test_rf = random_forest.predict (X_test)

# Calculate accuracy for decision tree
accuracy_train_dt = accuracy_score(Y_train, y_pred_train_dt)
accuracy_test_dt = accuracy_score(Y_test, y_pred_test_dt)

# Calculate accuracy for random forest
accuracy_train_rf = accuracy_score(Y_train, y_pred_train_rf)
accuracy_test_rf = accuracy_score(Y_test, y_pred_test_rf)

# Predict on the training set for logistic regression to compare training performance
y_pred_train_lr = logreg.predict(X_train)
accuracy_train_lr = accuracy_score(Y_train, y_pred_train_lr)

# Prepare data for plotting
models = ['Logistic Regression', 'Decision Tree', 'Random Forest']
accuracy_train = [accuracy_train_lr, accuracy_train_dt, accuracy_train_rf]
accuracy_test=[accuracy,accuracy_test_dt,accuracy_test_rf]
# 'accuracy' is already calculated for logistic regression

# Plotting
fig,ax= plt.subplots()
index = range (len(models))
bar_width = 0.35
opacity = 0.8

rects1 = ax.bar(index, accuracy_train, bar_width,
                alpha=opacity, color='b', label='Training Accuracy')

rects2 = ax.bar([p + bar_width for p in index], accuracy_test, bar_width,
                alpha=opacity, color='r', label ='Testing Accuracy')

ax.set_xlabel('Model')
ax.set_ylabel('Accuracy')
ax.set_title('Model Comparision by Trainning and Testing Accuracy')
ax.set_xticks([p + bar_width/2 for p in index])
ax.set_xticklabels(models)
ax.legend()

plt.tight_layout()
plt.show()

# Calculate AUC for logistic regression
auc_lr = roc_auc_score(Y_test, logreg.predict_proba(X_test) [:, 1])

# Calculate AUC for decision tree
auc_dt = roc_auc_score(Y_test, decision_tree.predict_proba(X_test) [:, 1])

# Calculate AUC for random forest
auc_rf = roc_auc_score(Y_test, random_forest.predict_proba(X_test) [:, 1])

print(f'AUC of Logistic Regression: {auc_lr}')
print(f'AUC of Decision Tree: {auc_dt}')
print(f'AUC of Random Forest: {auc_rf}')

from sklearn.tree import plot_tree

# Plot the decision tree
plt. figure(figsize=(20,10))
plot_tree(decision_tree, filled= True, rounded=True, class_names=['No Churn', 'Churn'],
          feature_names=list(X. columns) , max_depth=2)  # Limit depth for readability
plt. title("Decision Tree Visualization")
plt. show()

# Extract coefficients from the logistic regression model
coefficients = logreg.coef_[0]

# Create a DataFrame to visualize the coefficients (importance) of features
coefficients_df = pd.DataFrame({'Feature': X.columns, 'Coefficient' :
                                coefficients})

# Sort the DataFrame by the absolute values of coefficients for importance
coefficients_df = coefficients_df.sort_values(by='Coefficient', key=abs,
                                              ascending=False)

#Plotting the top 10 most impactful features from logistic regression
top_features_df = coefficients_df.head(10)

plt.figure(figsize= (10, 6))
sns.barplot(x='Coefficient', y='Feature', data=top_features_df,
            palette="pastel")
plt. title('Top 10 Feature Coefficients from Logistic Regression')
plt. xlabel( 'Coefficient Value')
plt. ylabel ('Feature')
plt. show()

!jupyter nbconvert --to html CustomerChurnPredicition_KanakSharma.ipynb

